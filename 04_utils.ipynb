{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T19:42:16.564548Z",
     "start_time": "2020-03-13T19:42:16.561729Z"
    }
   },
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:39:15.613552Z",
     "start_time": "2020-02-07T04:39:15.611097Z"
    }
   },
   "source": [
    "# Util functions for Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T19:58:55.983117Z",
     "start_time": "2020-03-16T19:58:55.864398Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "import torch\n",
    "from torch_lr_finder import LRFinder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def create_opt(lr,model):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    return opt\n",
    "\n",
    "def create_one_cycle(opt,max_lr,epochs,dataLoader):\n",
    "    return torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer=opt,\n",
    "        max_lr=max_lr,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=len(dataLoader))\n",
    "\n",
    "def find_lr(model,opt,loss_func,device,dataLoader):\n",
    "    lr_finder = LRFinder(model=model, optimizer=opt, criterion=loss_func, device=device)\n",
    "    lr_finder.range_test(dataLoader, end_lr=100, num_iter=200)\n",
    "    lr_finder.plot()\n",
    "    # reset model & opt to their original weights\n",
    "    lr_finder.reset()\n",
    "    \n",
    "def printNumModelParams(model):\n",
    "    layers_req_grad = 0\n",
    "    tot_layers = 0\n",
    "\n",
    "    params_req_grad = 0\n",
    "    tot_params = 0\n",
    "\n",
    "    for param in model.named_parameters():\n",
    "        #print(param[0])\n",
    "        if (param[1].requires_grad):\n",
    "            layers_req_grad += 1\n",
    "            params_req_grad += param[1].nelement()\n",
    "        tot_layers += 1\n",
    "        tot_params += param[1].nelement()\n",
    "    print(\"{0:,} layers require gradients (unfrozen) out of {1:,} layers\".format(layers_req_grad, tot_layers))\n",
    "    print(\"{0:,} parameters require gradients (unfrozen) out of {1:,} parameters\".format(params_req_grad, tot_params))\n",
    "    \n",
    "def calcAccuracy(preds, labels):\n",
    "    softedPreds = torch.softmax(preds,dim=1)\n",
    "    classPreds = softedPreds.argmax(dim=1)\n",
    "    totCorrect = (classPreds == labels).sum().item()\n",
    "    totNum = labels.nelement()\n",
    "    return totCorrect/totNum\n",
    "\n",
    "def rmse(preds, labels):\n",
    "    d = (preds - labels)**2\n",
    "    d = d.mean()\n",
    "    r = d.sqrt()\n",
    "    return r\n",
    "\n",
    "def writeMessage(msg, versionName):\n",
    "    # Write to file.\n",
    "    print(msg)\n",
    "    myFile = open(versionName+\".txt\", \"a\")\n",
    "    myFile.write(msg)\n",
    "    myFile.write(\"\\n\")\n",
    "    myFile.close()\n",
    "    \n",
    "def plotSample(X):\n",
    "        plt.figure(figsize=(20,20))\n",
    "        \n",
    "        plt.subplot(211)\n",
    "        title = 'Channel 0'\n",
    "        plt.title(title)\n",
    "        plt.imshow(X[0])\n",
    "        plt.colorbar()\n",
    "        \n",
    "        plt.subplot(212)\n",
    "        title = 'Channel 1'\n",
    "        plt.title(title)\n",
    "        plt.imshow(X[1])\n",
    "        plt.colorbar()\n",
    "    \n",
    "def plotSampleWpredictionByChannel(sample, prediction):\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    fig.set_size_inches(20,20, forward=True)\n",
    "\n",
    "    axs[0, 0].imshow(sample[0])\n",
    "    axs[0, 0].set_title('Simulated Channel 0')\n",
    "    axs[0, 1].imshow(prediction[0])\n",
    "    axs[0, 1].set_title('Predicted Channel 0]')\n",
    "    axs[1, 0].imshow(sample[1])\n",
    "    axs[1, 0].set_title('Simulated Channel 1')\n",
    "    axs[1, 1].imshow(prediction[1])\n",
    "    axs[1, 1].set_title('Predicted Channel 1')\n",
    "    #plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    # for ax in axs.flat:\n",
    "    #     ax.set(xlabel='x-label', ylabel='y-label')\n",
    "\n",
    "    # # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "    # for ax in axs.flat:\n",
    "    #     ax.label_outer()\n",
    "\n",
    "def plotSampleWprediction(sample,prediction):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    A = np.vstack([sample[0], sample[1]])\n",
    "    B = np.vstack([prediction[0], prediction[1]])\n",
    "    C = np.hstack([A,B])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(C)\n",
    "    plt.colorbar()\n",
    "\n",
    "    \n",
    "def curl(X,device='cpu'):\n",
    "    f1 = X[:,0,:,:]\n",
    "    f2 = X[:,1,:,:]\n",
    "    df1_dy = f1[:,1:,:] - f1[:,:-1,:]\n",
    "    df1_dy = torch.cat([df1_dy,torch.zeros((df1_dy.shape[0],1,f1.shape[2])).to(device)], axis=1) \n",
    "    df2_dx = f2[:,:,1:] - f2[:,:,:-1]\n",
    "    df2_dx = torch.cat([df2_dx,torch.zeros((f2.shape[0],f2.shape[1],1)).to(device)], axis=2)\n",
    "    c = df1_dy - df2_dx\n",
    "    c = c[:,None,:,:]\n",
    "    return c\n",
    "\n",
    "def jacobian(X,device='cpu'):\n",
    "    f1 = X[:,0,:,:]\n",
    "    f2 = X[:,1,:,:]\n",
    "    \n",
    "    df1_dx = f1[:,:,1:] - f1[:,:,:-1]\n",
    "    df1_dx = torch.cat([df1_dx,torch.zeros((f2.shape[0],f2.shape[1],1)).to(device)], axis=2)\n",
    "    \n",
    "    df1_dy = f1[:,1:,:] - f1[:,:-1,:]\n",
    "    df1_dy = torch.cat([df1_dy,torch.zeros((df1_dy.shape[0],1,f1.shape[2])).to(device)], axis=1) \n",
    "    \n",
    "    df2_dx = f2[:,:,1:] - f2[:,:,:-1]\n",
    "    df2_dx = torch.cat([df2_dx,torch.zeros((f2.shape[0],f2.shape[1],1)).to(device)], axis=2)\n",
    "\n",
    "    df2_dy = f2[:,1:,:] - f2[:,:-1,:]\n",
    "    df2_dy = torch.cat([df2_dy,torch.zeros((df1_dy.shape[0],1,f1.shape[2])).to(device)], axis=1) \n",
    "  \n",
    "    return torch.stack([df1_dx, df1_dy, df2_dx, df2_dy], axis=1)\n",
    "\n",
    "# http://farside.ph.utexas.edu/teaching/336L/Fluidhtml/node69.html\n",
    "# When creating the stream function, the second channel of X is not going to be used. \n",
    "# It's there so we don't have to change the AE model code. \n",
    "def stream2uv(X,device='cpu'):\n",
    "    u = X[:,0,1:,:] - X[:,0,:-1,:]\n",
    "    w = torch.unsqueeze(u[:,-1,:],axis=1)\n",
    "    u = torch.cat([u,w],axis=1)\n",
    "    v = X[:,0,:,1:] - X[:,0,:,:-1]\n",
    "    w = torch.unsqueeze(u[:,:,-1],axis=2)\n",
    "    v = torch.cat([v,w],axis=2)\n",
    "    return torch.stack([u,v], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Fluid's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T17:33:20.226643Z",
     "start_time": "2020-03-16T17:33:20.221726Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# This curl makes no sense to me. I think the derivative should be taken across channels\n",
    "# def curl(x, data_format='NHWC'):\n",
    "#     if data_format == 'NCHW': x = nchw_to_nhwc(x)\n",
    "\n",
    "#     u = x[:,1:,:,0] - x[:,:-1,:,0] # ds/dy\n",
    "#     v = x[:,:,:-1,0] - x[:,:,1:,0] # -ds/dx,\n",
    "#     u = tf.concat([u, tf.expand_dims(u[:,-1,:], axis=1)], axis=1)\n",
    "#     v = tf.concat([v, tf.expand_dims(v[:,:,-1], axis=2)], axis=2)\n",
    "#     c = tf.stack([u,v], axis=-1)\n",
    "\n",
    "#     if data_format == 'NCHW': c = nhwc_to_nchw(c)\n",
    "#     return c\n",
    "\n",
    "# def jacobian(x, data_format='NHCW'):\n",
    "#     if data_format == 'NCHW':\n",
    "#         x = nchw_to_nhwc(x)\n",
    "\n",
    "#     dudx = x[:,:,1:,0] - x[:,:,:-1,0]\n",
    "#     dudy = x[:,1:,:,0] - x[:,:-1,:,0]\n",
    "#     dvdx = x[:,:,1:,1] - x[:,:,:-1,1]\n",
    "#     dvdy = x[:,1:,:,1] - x[:,:-1,:,1]\n",
    "    \n",
    "#     dudx = tf.concat([dudx,tf.expand_dims(dudx[:,:,-1], axis=2)], axis=2)\n",
    "#     dvdx = tf.concat([dvdx,tf.expand_dims(dvdx[:,:,-1], axis=2)], axis=2)\n",
    "#     dudy = tf.concat([dudy,tf.expand_dims(dudy[:,-1,:], axis=1)], axis=1)\n",
    "#     dvdy = tf.concat([dvdy,tf.expand_dims(dvdy[:,-1,:], axis=1)], axis=1)\n",
    "\n",
    "#     j = tf.stack([dudx,dudy,dvdx,dvdy], axis=-1)\n",
    "#     w = tf.expand_dims(dvdx - dudy, axis=-1) # vorticity (for visualization)\n",
    "\n",
    "#     if data_format == 'NCHW':\n",
    "#         j = nhwc_to_nchw(j)\n",
    "#         w = nhwc_to_nchw(w)\n",
    "#     return j, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and How to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T17:33:26.910468Z",
     "start_time": "2020-03-16T17:33:20.941835Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T17:34:17.152139Z",
     "start_time": "2020-03-16T17:34:17.043096Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=28*28, n_informative=400, n_redundant=2, n_repeated=0, n_classes=2)\n",
    "X = X.astype('float32')\n",
    "X = torch.tensor(X).reshape(100,1,28,28).type(torch.float32)\n",
    "y = torch.tensor(y)\n",
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T17:34:17.551151Z",
     "start_time": "2020-03-16T17:34:17.543809Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:49:05.841088Z",
     "start_time": "2020-03-04T17:49:05.823056Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = MyDataset(X,y)\n",
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:49:06.666724Z",
     "start_time": "2020-03-04T17:49:06.663648Z"
    }
   },
   "outputs": [],
   "source": [
    "dataLoader = DataLoader(dataset,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:51:08.762923Z",
     "start_time": "2020-03-04T17:51:08.751229Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:51:09.253668Z",
     "start_time": "2020-03-04T17:51:09.205910Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:51:09.713437Z",
     "start_time": "2020-03-04T17:51:09.710212Z"
    }
   },
   "outputs": [],
   "source": [
    "max_lr = 1e-3\n",
    "epochs = 100\n",
    "opt = create_opt(max_lr,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:51:10.713030Z",
     "start_time": "2020-03-04T17:51:10.709964Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_sched = create_one_cycle(opt,max_lr,epochs,dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:51:17.554539Z",
     "start_time": "2020-03-04T17:51:17.549532Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataLoader))\n",
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:51:18.953216Z",
     "start_time": "2020-03-04T17:51:18.839241Z"
    }
   },
   "outputs": [],
   "source": [
    "out = model(batch[0])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:51:21.395057Z",
     "start_time": "2020-03-04T17:51:21.390544Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:51:22.664989Z",
     "start_time": "2020-03-04T17:51:22.534046Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_func(out,batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:55:09.834610Z",
     "start_time": "2020-03-04T17:55:09.831458Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:55:18.128263Z",
     "start_time": "2020-03-04T17:55:11.518064Z"
    }
   },
   "outputs": [],
   "source": [
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "    \n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:55:37.594283Z",
     "start_time": "2020-03-04T17:55:32.464603Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = create_opt(1e-7,model)\n",
    "device = 'cpu'\n",
    "find_lr(model,opt,loss_func,device,dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:55:55.460846Z",
     "start_time": "2020-03-04T17:55:55.456422Z"
    }
   },
   "outputs": [],
   "source": [
    "max_lr = 1e-4 # based on the above graph\n",
    "start_lr = 5e-6\n",
    "opt = create_opt(start_lr,model)\n",
    "lr_scheduler = create_one_cycle(opt,max_lr,epochs,dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:56:42.459942Z",
     "start_time": "2020-03-04T17:56:42.456040Z"
    }
   },
   "outputs": [],
   "source": [
    "printNumModelParams(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T18:44:22.151066Z",
     "start_time": "2020-03-04T18:44:22.076628Z"
    }
   },
   "outputs": [],
   "source": [
    "LRFinder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T17:34:29.976810Z",
     "start_time": "2020-03-16T17:34:29.967347Z"
    }
   },
   "outputs": [],
   "source": [
    "bz = 8\n",
    "h = 4\n",
    "w = 3\n",
    "c = 2\n",
    "x = torch.rand(bz,c,h,w )\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T17:34:30.439345Z",
     "start_time": "2020-03-16T17:34:30.430778Z"
    }
   },
   "outputs": [],
   "source": [
    "a= curl(x)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T17:34:31.710151Z",
     "start_time": "2020-03-16T17:34:31.703426Z"
    }
   },
   "outputs": [],
   "source": [
    "J = jacobian(x)\n",
    "print(J.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T17:34:59.807624Z",
     "start_time": "2020-03-16T17:34:59.728616Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T17:53:01.047168Z",
     "start_time": "2020-03-16T17:53:01.042363Z"
    }
   },
   "outputs": [],
   "source": [
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T17:53:06.643040Z",
     "start_time": "2020-03-16T17:53:06.634894Z"
    }
   },
   "outputs": [],
   "source": [
    "a = curl(x,device)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T19:22:35.878816Z",
     "start_time": "2020-03-16T19:22:35.858499Z"
    }
   },
   "outputs": [],
   "source": [
    "J = jacobian(x,device)\n",
    "print(J.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T20:01:03.708227Z",
     "start_time": "2020-03-16T20:01:03.701058Z"
    }
   },
   "outputs": [],
   "source": [
    "stream2uv(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T20:01:04.023888Z",
     "start_time": "2020-03-16T20:01:04.016760Z"
    }
   },
   "outputs": [],
   "source": [
    "stream2uv(X,device).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T20:01:20.823182Z",
     "start_time": "2020-03-16T20:01:20.808794Z"
    }
   },
   "outputs": [],
   "source": [
    "stream2uv(X) - stream2uv(X,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
